{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":30892,"databundleVersionId":2715462,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary\n!pip install torchgeometry","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-12T02:18:15.532823Z","iopub.execute_input":"2025-04-12T02:18:15.533050Z","iopub.status.idle":"2025-04-12T02:18:34.037480Z","shell.execute_reply.started":"2025-04-12T02:18:15.533028Z","shell.execute_reply":"2025-04-12T02:18:34.036332Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Obtaining dependency information for torchsummary from https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl.metadata\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nCollecting torchgeometry\n  Obtaining dependency information for torchgeometry from https://files.pythonhosted.org/packages/a6/d6/3f6820c0589bc3876080c59b58a3bad11af746a7b46f364b1cde7972bd72/torchgeometry-0.1.2-py2.py3-none-any.whl.metadata\n  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\nDownloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torchgeometry\nSuccessfully installed torchgeometry-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from torchsummary import summary\nfrom torchgeometry.losses import one_hot\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport time\nimport imageio\nimport matplotlib.pyplot as plt\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\nfrom collections import OrderedDict\nfrom PIL import ImageFilter\nimport random\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:18:34.039443Z","iopub.execute_input":"2025-04-12T02:18:34.039737Z","iopub.status.idle":"2025-04-12T02:18:39.573905Z","shell.execute_reply.started":"2025-04-12T02:18:34.039697Z","shell.execute_reply":"2025-04-12T02:18:39.573240Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:18:39.575046Z","iopub.execute_input":"2025-04-12T02:18:39.575386Z","iopub.status.idle":"2025-04-12T02:18:40.604738Z","shell.execute_reply.started":"2025-04-12T02:18:39.575354Z","shell.execute_reply":"2025-04-12T02:18:40.603654Z"},"trusted":true},"outputs":[{"name":"stdout","text":"GPU 0: Tesla T4 (UUID: GPU-79bab146-330f-56c7-3cca-c7a110ca9fff)\nGPU 1: Tesla T4 (UUID: GPU-f02f0cd1-458b-2063-6a54-d8cc6ff8d9bf)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:18:40.606359Z","iopub.execute_input":"2025-04-12T02:18:40.606740Z","iopub.status.idle":"2025-04-12T02:18:40.675750Z","shell.execute_reply.started":"2025-04-12T02:18:40.606705Z","shell.execute_reply":"2025-04-12T02:18:40.674743Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Number of class in the data set (3: neoplastic, non neoplastic, background)\nnum_classes = 3\n\n# Number of epoch\nepochs = 50\n\n# Hyperparameters for training \nlearning_rate = 2e-04\nbatch_size = 4\ndisplay_step = 50\n\n# Model path\ncheckpoint_path = '/kaggle/working/unet_model.pth'\npretrained_path = \"/kaggle/input/unet-checkpoint/unet_model.pth\"\n\n# Initialize lists to keep track of loss and accuracy\nloss_epoch_array = []\ntrain_accuracy = []\ntest_accuracy = []\nvalid_accuracy = []","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:19:58.516797Z","iopub.execute_input":"2025-04-12T02:19:58.517166Z","iopub.status.idle":"2025-04-12T02:19:58.522230Z","shell.execute_reply.started":"2025-04-12T02:19:58.517138Z","shell.execute_reply":"2025-04-12T02:19:58.521310Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transform = Compose([Resize((400, 560), interpolation=InterpolationMode.BILINEAR),\n                     PILToTensor()])","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:20:00.064216Z","iopub.execute_input":"2025-04-12T02:20:00.065164Z","iopub.status.idle":"2025-04-12T02:20:00.070238Z","shell.execute_reply.started":"2025-04-12T02:20:00.065121Z","shell.execute_reply":"2025-04-12T02:20:00.069271Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class UNetDataClass(Dataset):\n    def __init__(self, images_path, masks_path, transform, blur_radius=3, rotation_angle = 15):\n        super(UNetDataClass, self).__init__()\n        \n        images_list = os.listdir(images_path)\n        masks_list = os.listdir(masks_path)\n        \n        images_list = [images_path + image_name for image_name in images_list]\n        masks_list = [masks_path + mask_name for mask_name in masks_list]\n        \n        self.images_list = images_list\n        self.masks_list = masks_list\n        self.transform = transform\n        self.blur_radius = blur_radius\n        self.rotation_angle = rotation_angle\n    def __getitem__(self, index):\n        img_path = self.images_list[index]\n        mask_path = self.masks_list[index]\n        \n        # Open image and mask\n        data = Image.open(img_path)\n        label = Image.open(mask_path)\n        \n        #Additional Gaussian blur\n        data = data.filter(ImageFilter.GaussianBlur(self.blur_radius))\n        \n        # Additional Random Rotation\n        if self.rotation_angle > 0:\n            random_angle = random.uniform(-self.rotation_angle, self.rotation_angle)\n            data = data.rotate(random_angle)\n            label = label.rotate(random_angle)\n        \n        # Normalize\n        data = self.transform(data) / 255\n        label = self.transform(label) / 255\n        \n        label = torch.where(label>0.65, 1.0, 0.0)\n        \n        label[2, :, :] = 0.0001\n        label = torch.argmax(label, 0).type(torch.int64)\n        \n        return data, label\n    \n    def __len__(self):\n        return len(self.images_list)","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:20:07.788731Z","iopub.execute_input":"2025-04-12T02:20:07.789061Z","iopub.status.idle":"2025-04-12T02:20:07.798120Z","shell.execute_reply.started":"2025-04-12T02:20:07.789035Z","shell.execute_reply":"2025-04-12T02:20:07.797185Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"images_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\nmasks_path =  \"/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt/\"","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:20:08.210964Z","iopub.execute_input":"2025-04-12T02:20:08.211541Z","iopub.status.idle":"2025-04-12T02:20:08.215127Z","shell.execute_reply.started":"2025-04-12T02:20:08.211516Z","shell.execute_reply":"2025-04-12T02:20:08.214231Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"unet_dataset = UNetDataClass(images_path, masks_path, transform)","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:20:10.715237Z","iopub.execute_input":"2025-04-12T02:20:10.715876Z","iopub.status.idle":"2025-04-12T02:20:10.861998Z","shell.execute_reply.started":"2025-04-12T02:20:10.715848Z","shell.execute_reply":"2025-04-12T02:20:10.861394Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_size = 0.8\nvalid_size = 0.2","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:20:11.209376Z","iopub.execute_input":"2025-04-12T02:20:11.209915Z","iopub.status.idle":"2025-04-12T02:20:11.213427Z","shell.execute_reply.started":"2025-04-12T02:20:11.209891Z","shell.execute_reply":"2025-04-12T02:20:11.212586Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_set, valid_set = random_split(unet_dataset, \n                                    [int(train_size * len(unet_dataset)) , \n                                     int(valid_size * len(unet_dataset))])","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:20:12.604905Z","iopub.execute_input":"2025-04-12T02:20:12.605715Z","iopub.status.idle":"2025-04-12T02:20:12.625623Z","shell.execute_reply.started":"2025-04-12T02:20:12.605680Z","shell.execute_reply":"2025-04-12T02:20:12.624823Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2025-04-12T02:20:14.734683Z","iopub.execute_input":"2025-04-12T02:20:14.735490Z","iopub.status.idle":"2025-04-12T02:20:14.739976Z","shell.execute_reply.started":"2025-04-12T02:20:14.735456Z","shell.execute_reply":"2025-04-12T02:20:14.739135Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class encoder_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(encoder_block, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n        \n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.3)\n        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.dropout(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        \n        next_layer = self.max_pool(x)\n        skip_layer = x\n        \n        return next_layer, skip_layer","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.905761Z","iopub.execute_input":"2023-11-15T14:45:50.906016Z","iopub.status.idle":"2023-11-15T14:45:50.914565Z","shell.execute_reply.started":"2023-11-15T14:45:50.905993Z","shell.execute_reply":"2023-11-15T14:45:50.913736Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class decoder_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(decoder_block, self).__init__()\n        \n        self.transpose_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n        \n        self.conv1 = nn.Conv2d(2 * out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n        \n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.relu = nn.ReLU() \n        self.dropout = nn.Dropout(p=0.3)\n    \n    def forward(self, x, skip_layer):\n        x = self.transpose_conv(x)\n        x = torch.cat([x, skip_layer], axis=1)\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.dropout(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.91559Z","iopub.execute_input":"2023-11-15T14:45:50.915849Z","iopub.status.idle":"2023-11-15T14:45:50.92524Z","shell.execute_reply.started":"2023-11-15T14:45:50.915816Z","shell.execute_reply":"2023-11-15T14:45:50.924312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class bottleneck_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(bottleneck_block, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n        \n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.3)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.dropout(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.926397Z","iopub.execute_input":"2023-11-15T14:45:50.926716Z","iopub.status.idle":"2023-11-15T14:45:50.938142Z","shell.execute_reply.started":"2023-11-15T14:45:50.926691Z","shell.execute_reply":"2023-11-15T14:45:50.937397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# UNet model\nclass UNet(nn.Module):\n    def __init__(self, n_class=3):\n        super(UNet, self).__init__()\n        # Encoder blocks\n        self.enc1 = encoder_block(3, 64)\n        self.enc2 = encoder_block(64, 128)\n        self.enc3 = encoder_block(128, 256)\n        self.enc4 = encoder_block(256, 512)\n        \n        # Bottleneck block\n        self.bottleneck = bottleneck_block(512, 1024)\n        \n        # Decoder blocks\n        self.dec1 = decoder_block(1024, 512)\n        self.dec2 = decoder_block(512, 256)\n        self.dec3 = decoder_block(256, 128)\n        self.dec4 = decoder_block(128, 64)\n        \n        # 1x1 convolution\n        self.out = nn.Conv2d(64, n_class, kernel_size=1, padding='same')\n        \n    def forward(self, image):\n        n1, s1 = self.enc1(image)\n        n2, s2 = self.enc2(n1)\n        n3, s3 = self.enc3(n2)\n        n4, s4 = self.enc4(n3)\n        \n        n5 = self.bottleneck(n4)\n        \n        n6 = self.dec1(n5, s4)\n        n7 = self.dec2(n6, s3)\n        n8 = self.dec3(n7, s2)\n        n9 = self.dec4(n8, s1)\n        \n        output = self.out(n9)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.939357Z","iopub.execute_input":"2023-11-15T14:45:50.940037Z","iopub.status.idle":"2023-11-15T14:45:50.950441Z","shell.execute_reply.started":"2023-11-15T14:45:50.940004Z","shell.execute_reply":"2023-11-15T14:45:50.949448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CEDiceLoss(nn.Module):\n    def __init__(self, weights) -> None:\n        super(CEDiceLoss, self).__init__()\n        self.eps: float = 1e-6\n        self.weights: torch.Tensor = weights\n\n    def forward(\n            self,\n            input: torch.Tensor,\n            target: torch.Tensor) -> torch.Tensor:\n        if not torch.is_tensor(input):\n            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n                             .format(input.shape))\n        if not input.shape[-2:] == target.shape[-2:]:\n            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n                             .format(input.shape, input.shape))\n        if not input.device == target.device:\n            raise ValueError(\n                \"input and target must be in the same device. Got: {}\" .format(\n                    input.device, target.device))\n        if not self.weights.shape[1] == input.shape[1]:\n            raise ValueError(\"The number of weights must equal the number of classes\")\n        if not torch.sum(self.weights).item() == 1:\n            raise ValueError(\"The sum of all weights must equal 1\")\n            \n        # cross entropy loss\n        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n        \n        # compute softmax over the classes axis\n        input_soft = F.softmax(input, dim=1)\n\n        # create the labels one hot tensor\n        target_one_hot = one_hot(target, num_classes=input.shape[1],\n                                 device=input.device, dtype=input.dtype)\n\n        # compute the actual dice score\n        dims = (2, 3)\n        intersection = torch.sum(input_soft * target_one_hot, dims)\n        cardinality = torch.sum(input_soft + target_one_hot, dims)\n\n        dice_score = 2. * intersection / (cardinality + self.eps)\n        \n        dice_score = torch.sum(dice_score * self.weights, dim=1)\n        \n        return torch.mean(1. - dice_score) + celoss","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.951743Z","iopub.execute_input":"2023-11-15T14:45:50.952061Z","iopub.status.idle":"2023-11-15T14:45:50.967011Z","shell.execute_reply.started":"2023-11-15T14:45:50.952035Z","shell.execute_reply":"2023-11-15T14:45:50.966048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def weights_init(model):\n    if isinstance(model, nn.Linear):\n        # Xavier Distribution\n        torch.nn.init.xavier_uniform_(model.weight)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.968131Z","iopub.execute_input":"2023-11-15T14:45:50.968407Z","iopub.status.idle":"2023-11-15T14:45:50.978853Z","shell.execute_reply.started":"2023-11-15T14:45:50.968383Z","shell.execute_reply":"2023-11-15T14:45:50.97804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(model, optimizer, path):\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, path)\n\ndef load_model(model, optimizer, path):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return model, optimizer","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.979916Z","iopub.execute_input":"2023-11-15T14:45:50.980205Z","iopub.status.idle":"2023-11-15T14:45:50.987966Z","shell.execute_reply.started":"2023-11-15T14:45:50.980163Z","shell.execute_reply":"2023-11-15T14:45:50.987038Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train function for each epoch\ndef train(train_dataloader, valid_dataloader,learing_rate_scheduler, epoch, display_step):\n    # Training loop, need to write to return the loss and the train and test in each epoch\n    # IMPLEMENT\n    return train_loss_epoch , test_loss_epoch","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:50.989265Z","iopub.execute_input":"2023-11-15T14:45:50.989584Z","iopub.status.idle":"2023-11-15T14:45:51.000433Z","shell.execute_reply.started":"2023-11-15T14:45:50.989548Z","shell.execute_reply":"2023-11-15T14:45:50.99945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test function\ndef test(dataloader):\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for i, (data, targets) in enumerate(dataloader):\n            data, targets = data.to(device), targets.to(device)\n            outputs = model(data)\n            _, pred = torch.max(outputs, 1)\n            test_loss += targets.size(0)\n            correct += torch.sum(pred == targets).item()\n    return 100.0 * correct / test_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:51.001675Z","iopub.execute_input":"2023-11-15T14:45:51.002438Z","iopub.status.idle":"2023-11-15T14:45:51.012012Z","shell.execute_reply.started":"2023-11-15T14:45:51.002413Z","shell.execute_reply":"2023-11-15T14:45:51.011258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet()\nmodel.apply(weights_init)\nmodel = nn.DataParallel(model)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:51.014127Z","iopub.execute_input":"2023-11-15T14:45:51.014819Z","iopub.status.idle":"2023-11-15T14:45:54.524653Z","shell.execute_reply.started":"2023-11-15T14:45:51.01477Z","shell.execute_reply":"2023-11-15T14:45:54.523779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\nloss_function = CEDiceLoss(weights)\n\n# Define the optimizer (Adam optimizer)\noptimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n#optimizer.load_state_dict(checkpoint['optimizer'])\n\n# Learning rate scheduler\nlearing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:54.52574Z","iopub.execute_input":"2023-11-15T14:45:54.526011Z","iopub.status.idle":"2023-11-15T14:45:54.532905Z","shell.execute_reply.started":"2023-11-15T14:45:54.525986Z","shell.execute_reply":"2023-11-15T14:45:54.5319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_model(model, optimizer, checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:54.533991Z","iopub.execute_input":"2023-11-15T14:45:54.534255Z","iopub.status.idle":"2023-11-15T14:45:54.748067Z","shell.execute_reply.started":"2023-11-15T14:45:54.534232Z","shell.execute_reply":"2023-11-15T14:45:54.747057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(\n    # set the wandb project where this run will be logged\n#     project= \"PolypSegment\", \n    key = \"f7219850f9e48723af964abfa85e5cc88f0c29b3\",\n)\nwandb.init(\n    project = \"BKAI-IGH_NeoPolyp\"\n)\n# Training loop\ntrain_loss_array = []\ntest_loss_array = []\nlast_loss = 9999999999999\nfor epoch in range(epochs):\n    train_loss_epoch = 0\n    test_loss_epoch = 0\n    (train_loss_epoch, test_loss_epoch) = train(train_dataloader, \n                                              valid_dataloader, \n                                              learing_rate_scheduler, epoch, display_step)\n    \n    if test_loss_epoch < last_loss:\n        save_model(model, optimizer, checkpoint_path)\n        last_loss = test_loss_epoch\n        \n    learing_rate_scheduler.step()\n    train_loss_array.append(train_loss_epoch)\n    test_loss_array.append(test_loss_epoch)\n    wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:45:54.749436Z","iopub.execute_input":"2023-11-15T14:45:54.749822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 90\nplt.rcParams['figure.figsize'] = (6, 4)\nepochs_array = range(epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Training and Test loss\nplt.plot(epochs_array, train_loss_array, 'g', label='Training loss')\n# plt.plot(epochs_array, test_loss_array, 'b', label='Test loss')\nplt.title('Training and Test loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, (data, label) in enumerate(train_dataloader):\n    img = data\n    mask = label\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, arr = plt.subplots(4, 3, figsize=(16, 12))\narr[0][0].set_title('Image')\narr[0][1].set_title('Segmentation')\narr[0][2].set_title('Predict')\n\nmodel.eval()\nwith torch.no_grad():\n    predict = model(img)\n\nfor i in range(4):\n    arr[i][0].imshow(img[i].permute(1, 2, 0));\n    \n    arr[i][1].imshow(F.one_hot(mask[i]).float())\n    \n    arr[i][2].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = Compose([Resize((400, 560), interpolation=InterpolationMode.BILINEAR),\n                     PILToTensor()])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNetTestDataClass(Dataset):\n    def __init__(self, images_path, transform):\n        super(UNetTestDataClass, self).__init__()\n        \n        images_list = os.listdir(images_path)\n        images_list = [images_path+i for i in images_list]\n        \n        self.images_list = images_list\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        img_path = self.images_list[index]\n        data = Image.open(img_path)\n        h = data.size[1]\n        w = data.size[0]\n        data = self.transform(data) / 255        \n        return data, img_path, h, w\n    \n    def __len__(self):\n        return len(self.images_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = '/kaggle/input/bkai-igh-neopolyp/test/test/'\nunet_test_dataset = UNetTestDataClass(path, transform)\ntest_dataloader = DataLoader(unet_test_dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, (data, path, h, w) in enumerate(test_dataloader):\n    img = data\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, arr = plt.subplots(5, 2, figsize=(16, 12))\narr[0][0].set_title('Image');\narr[0][1].set_title('Predict');\n\nmodel.eval()\nwith torch.no_grad():\n    predict = model(img)\n\nfor i in range(5):\n    arr[i][0].imshow(img[i].permute(1, 2, 0));\n    arr[i][1].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nif not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n    os.mkdir(\"/kaggle/working/predicted_masks\")\nfor _, (img, path, H, W) in enumerate(test_dataloader):\n    a = path\n    b = img\n    h = H\n    w = W\n    \n    with torch.no_grad():\n        predicted_mask = model(b)\n    for i in range(len(a)):\n        image_id = a[i].split('/')[-1].split('.')[0]\n        filename = image_id + \".png\"\n        mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 0] = 255\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef mask2string(dir):\n    ## mask --> string\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\ndf.to_csv(r'result.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}